#!/usr/bin/env python3
"""
Training Dataset Summary & Statistics Report
Analyzes the generated fraud detection training dataset
"""

import json
import csv
from pathlib import Path
from collections import defaultdict

def analyze_json_dataset(json_file):
    """Analyze JSON dataset structure"""
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    print("\n" + "="*80)
    print("  TRAINING DATASET SUMMARY & STATISTICS")
    print("="*80 + "\n")
    
    print(f"ğŸ“Š Dataset Generated: {json_file}")
    print(f"   File Size: {json_file.stat().st_size:,} bytes")
    print(f"   Format: JSON (structured) + CSV (analytics)\n")
    
    # Scenario statistics
    print("ğŸ“‹ SCENARIOS TESTED:")
    print("-" * 80)
    
    scenario_stats = defaultdict(lambda: {
        'turns': 0,
        'scams_detected': 0,
        'avg_confidence': 0,
        'confidence_scores': []
    })
    
    total_turns = 0
    total_scams = 0
    
    for scenario in data:
        scenario_name = scenario['scenario']
        scenario_type = scenario['type']
        channel = scenario['channel']
        num_turns = len(scenario['turns'])
        
        scams_in_scenario = sum(1 for turn in scenario['turns'] if turn['scam_detected'])
        confidences = [turn['confidence'] for turn in scenario['turns']]
        avg_conf = sum(confidences) / len(confidences) if confidences else 0
        
        print(f"\n  âœ“ {scenario_type}")
        print(f"    â””â”€ ID: {scenario_name}")
        print(f"    â””â”€ Channel: {channel}")
        print(f"    â””â”€ Turns: {num_turns}")
        print(f"    â””â”€ Scams Detected: {scams_in_scenario}/{num_turns}")
        print(f"    â””â”€ Avg Confidence: {avg_conf:.3f}")
        print(f"    â””â”€ Sample Conversation:")
        print(f"       Turn 1 (Early): \"{scenario['turns'][0]['victim_reply']}\"")
        mid = len(scenario['turns']) // 2
        print(f"       Turn {mid} (Middle): \"{scenario['turns'][mid-1]['victim_reply']}\"")
        print(f"       Turn {num_turns} (Late): \"{scenario['turns'][-1]['victim_reply']}\"")
        
        total_turns += num_turns
        total_scams += scams_in_scenario
        scenario_stats[scenario_name]['turns'] = num_turns
        scenario_stats[scenario_name]['scams_detected'] = scams_in_scenario
        scenario_stats[scenario_name]['confidence_scores'] = confidences
    
    # Overall statistics
    print("\n" + "="*80)
    print("ğŸ“ˆ OVERALL STATISTICS:")
    print("-" * 80)
    
    all_confidences = []
    for stats in scenario_stats.values():
        all_confidences.extend(stats['confidence_scores'])
    
    avg_overall_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0
    max_confidence = max(all_confidences) if all_confidences else 0
    min_confidence = min(all_confidences) if all_confidences else 0
    
    print(f"\nTotal Scenarios: {len(data)}")
    print(f"Total Conversation Turns: {total_turns}")
    print(f"Scams Detected: {total_scams}/{total_turns}")
    print(f"Detection Rate: {(total_scams/total_turns*100):.1f}%")
    print(f"\nConfidence Scores:")
    print(f"  â€¢ Average: {avg_overall_confidence:.3f}")
    print(f"  â€¢ Maximum: {max_confidence:.3f}")
    print(f"  â€¢ Minimum: {min_confidence:.3f}")
    
    # Conversation progression analysis
    print("\n" + "="*80)
    print("ğŸ”„ CONVERSATION PROGRESSION ANALYSIS:")
    print("-" * 80)
    
    print("\n  Turn 1 (Early Stage - Victim Skepticism):")
    for scenario in data:
        victim_msg = scenario['turns'][0]['victim_reply']
        print(f"    â€¢ {victim_msg}")
    
    print("\n  Middle Turns (Victim Engagement):")
    for scenario in data:
        mid = len(scenario['turns']) // 2
        if mid > 0:
            victim_msg = scenario['turns'][mid-1]['victim_reply']
            print(f"    â€¢ {victim_msg}")
    
    print("\n  Final Turns (Victim Compliance):")
    for scenario in data:
        victim_msg = scenario['turns'][-1]['victim_reply']
        print(f"    â€¢ {victim_msg}")
    
    # Dataset use cases
    print("\n" + "="*80)
    print("ğŸ¯ TRAINING DATASET USE CASES:")
    print("-" * 80)
    print("""
  1. Fraud Detection Model Training
     â€¢ Multi-turn scam conversation patterns
     â€¢ Victim engagement progression detection
     â€¢ Confidence scoring calibration
  
  2. Scammer Tactic Analysis
     â€¢ KYC expiry threats
     â€¢ Reward/cashback lures
     â€¢ Urgency and pressure tactics
     â€¢ Authority impersonation
  
  3. Conversation Flow Understanding
     â€¢ Early-stage skepticism patterns
     â€¢ Engagement and trust-building phases
     â€¢ Compliance requests sequence
     â€¢ Victim response consistency
  
  4. Model Evaluation
     â€¢ {0} labeled scam turns
     â€¢ {1} unlabeled turns (multi-class)
     â€¢ {2} total conversations
     â€¢ Real victim response patterns
    """.format(total_scams, total_turns - total_scams, len(data)))
    
    # Files created
    print("="*80)
    print("ğŸ“ OUTPUT FILES:")
    print("-" * 80)
    csv_file = Path('training_dataset.csv')
    json_file = Path('training_dataset.json')
    print(f"\n  1. training_dataset.json ({json_file.stat().st_size:,} bytes)")
    print(f"     â”œâ”€ Structured format for ML pipelines")
    print(f"     â”œâ”€ Complete conversation objects")
    print(f"     â””â”€ Per-turn scam detection flags")
    print(f"\n  2. training_dataset.csv ({csv_file.stat().st_size:,} bytes)")
    print(f"     â”œâ”€ Analytics-friendly format")
    print(f"     â”œâ”€ One row per conversation turn")
    print(f"     â””â”€ Easy import to pandas/sklearn")
    
    print("\n" + "="*80)
    print("âœ… Dataset ready for fraud detection model training!")
    print("="*80 + "\n")

if __name__ == "__main__":
    json_file = Path('training_dataset.json')
    if json_file.exists():
        analyze_json_dataset(json_file)
    else:
        print("training_dataset.json not found!")
